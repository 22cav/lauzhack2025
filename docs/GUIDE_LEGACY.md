# Complete Setup Guide - Gesture Control System

Welcome! This guide will help you set up and run the webcam-based gesture control system from scratch.

## üìã Table of Contents

- [Prerequisites](#prerequisites)
- [Quick Start](#quick-start)
- [Detailed Setup](#detailed-setup)
- [Testing the System](#testing-the-system)
- [Customizing Gestures](#customizing-gestures)
- [Troubleshooting](#troubleshooting)
- [FAQ](#faq)

---

## Prerequisites

### Required Software

| Software | Version | Check Command | Download Link |
|----------|---------|---------------|---------------|
| Python | 3.8+ | `python --version` | [python.org](https://www.python.org/downloads/) |
| Conda | Any | `conda --version` | [Anaconda](https://www.anaconda.com/) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html) |
| Webcam | Built-in or USB | - | - |

### Operating System Notes

- **macOS**: Fully supported (tested)
- **Windows**: Supported (modify key names in code)
- **Linux**: Supported (may need custom key bindings)

---

## Quick Start

For experienced users, here's the fastest path:

```bash
# 1. Create and activate conda environment
conda create -n lauzhack python=3.9 -y
conda activate lauzhack

# 2. Install dependencies
cd GestureControlPlugin/gesture_engine
pip install -r requirements.txt

# 3. Run the standalone application
python gesture_control_standalone.py
```

**Expected result**: Camera window opens showing your webcam feed with gesture recognition active.

---

## Detailed Setup

### Step 1: Clone the Repository (if not done)

```bash
git clone <repository-url>
cd lauzhack
```

### Step 2: Create Conda Environment

Conda provides isolated Python environments, preventing dependency conflicts.

```bash
# Create environment named 'lauzhack' with Python 3.9
conda create -n lauzhack python=3.9 -y

# Activate the environment
conda activate lauzhack
```

> [!TIP]
> Add `conda activate lauzhack` to your shell profile to auto-activate on terminal start.

**Verify activation**: Your terminal prompt should show `(lauzhack)` at the beginning.

### Step 3: Install Python Dependencies

Navigate to the gesture engine directory:

```bash
cd GestureControlPlugin/gesture_engine
```

Install required packages:

```bash
pip install -r requirements.txt
```

**What's being installed:**
- `mediapipe`: Hand and pose tracking AI model
- `opencv-python`: Camera capture and image processing
- `pyautogui`: Keyboard/mouse control for system actions

**Installation time**: ~2-5 minutes depending on internet speed.

### Step 4: Test Camera Access

Before running the full system, verify your camera works:

```bash
python -c "import cv2; cap = cv2.VideoCapture(0); ret, _ = cap.read(); cap.release(); print('‚úÖ Camera OK' if ret else '‚ùå Camera Error')"
```

If you see `‚ùå Camera Error`, try:
- Close other apps using the camera (Zoom, Skype, etc.)
- Try a different camera index: `cv2.VideoCapture(1)`
- Check camera permissions in System Preferences (macOS)

---

## Testing the System

### Running the Standalone Application

```bash
# Make sure you're in the gesture_engine directory
cd GestureControlPlugin/gesture_engine

# Activate conda environment (if not already active)
conda activate lauzhack

# Run the standalone gesture control system
python gesture_control_standalone.py
```

### What to Expect

**Terminal output:**
```
============================================================
üéÆ Standalone Gesture Control System
============================================================

üìπ Starting camera...
‚úÖ Camera initialized successfully

üñêÔ∏è  Gesture Controls:
   OPEN PALM  ‚Üí Volume Up
   CLOSED FIST ‚Üí Volume Down
   POINTING   ‚Üí Play/Pause

‚å®Ô∏è  Press ESC to exit

============================================================
```

**Camera window:**
- Live webcam feed
- Green skeleton overlay (pose tracking)
- Red hand landmarks (hand tracking)
- Current gesture displayed at top
- Last action performed
- FPS counter

### Testing Each Gesture

#### 1. **OPEN PALM** üñêÔ∏è - Volume Up

**How to perform:**
1. Face your palm toward the camera
2. Extend all four fingers (index, middle, ring, pinky)
3. Keep fingers clearly separated

**Expected behavior:**
- Window shows "Gesture: OPEN_PALM"
- "Last Action: Volume Up" appears
- System volume increases
- Terminal prints: `üîä Action: Volume Up`

**Troubleshooting:**
- Ensure good lighting
- Keep hand within camera frame (center works best)
- Make sure all fingers are clearly extended

---

#### 2. **CLOSED FIST** ‚úä - Volume Down

**How to perform:**
1. Close all fingers into a fist
2. Keep thumb tucked or extended (doesn't matter)

**Expected behavior:**
- Window shows "Gesture: CLOSED_FIST"
- "Last Action: Volume Down" appears
- System volume decreases
- Terminal prints: `üîâ Action: Volume Down`

---

#### 3. **POINTING** üëÜ - Play/Pause

**How to perform:**
1. Extend only your index finger
2. Keep other fingers closed
3. Point upward or forward

**Expected behavior:**
- Window shows "Gesture: POINTING"
- "Last Action: Play/Pause" appears
- Media playback toggles
- Terminal prints: `‚èØÔ∏è  Action: Play/Pause`

**Note**: This works with Spotify, Apple Music, YouTube (in browser), and most media apps.

---

### Gesture Cooldown

To prevent accidental repeated triggers, there's a **1-second cooldown** between actions. After triggering an action, you must wait 1 second before the next gesture is registered.

### Exiting the Application

Press **ESC** key while the camera window is focused.

---

## Customizing Gestures

Want to add your own gestures or change the actions? Here's how:

### Changing Actions

Edit `gesture_control_standalone.py`, find the `handle_gesture_action()` function:

```python
def handle_gesture_action(gesture):
    global last_action_time
    
    current_time = time.time()
    
    if current_time - last_action_time < GESTURE_COOLDOWN:
        return
    
    if gesture == "OPEN_PALM":
        # MODIFY THIS: Change 'volumeup' to any key
        pyautogui.press('volumeup')
        print("üîä Action: Volume Up")
        last_action_time = current_time
```

**Available key names** (use with `pyautogui.press()`):
- Media: `'playpause'`, `'nexttrack'`, `'prevtrack'`
- Volume: `'volumeup'`, `'volumedown'`, `'volumemute'`
- Brightness: `'brightnessup'`, `'brightnessdown'`
- Special: `'space'`, `'enter'`, `'esc'`, `'tab'`
- Letters: `'a'`, `'b'`, etc.

**Keyboard shortcuts** (use with `pyautogui.hotkey()`):
```python
# Example: Copy (Cmd+C on Mac, Ctrl+C on Windows)
pyautogui.hotkey('command', 'c')  # Mac
pyautogui.hotkey('ctrl', 'c')     # Windows
```

### Adding New Gestures

1. **Define detection logic** in `detect_gesture()`:

```python
def detect_gesture(hand_landmarks):
    # ... existing code ...
    
    # NEW: Add your custom gesture
    # Example: "Peace Sign" (index + middle finger extended)
    if extended_fingers == 2:
        index_up = hand_landmarks.landmark[mp_holistic.HandLandmark.INDEX_FINGER_TIP].y < \
                   hand_landmarks.landmark[mp_holistic.HandLandmark.INDEX_FINGER_PIP].y
        middle_up = hand_landmarks.landmark[mp_holistic.HandLandmark.MIDDLE_FINGER_TIP].y < \
                    hand_landmarks.landmark[mp_holistic.HandLandmark.MIDDLE_FINGER_PIP].y
        
        if index_up and middle_up:
            return "PEACE_SIGN"
    
    return "UNKNOWN"
```

2. **Add action handler**:

```python
def handle_gesture_action(gesture):
    # ... existing code ...
    
    elif gesture == "PEACE_SIGN":
        pyautogui.press('nexttrack')
        print("‚è≠Ô∏è  Action: Next Track")
        last_action_time = current_time
```

3. **Update display text** in `main()`:

```python
if current_gesture == "PEACE_SIGN":
    last_action = "Next Track"
```

### Adjusting Sensitivity

Edit `config.py` to tune detection sensitivity:

```python
# Lower = more sensitive, but may detect false positives
MIN_DETECTION_CONFIDENCE = 0.5  # Default: 0.5, Range: 0.0-1.0
MIN_TRACKING_CONFIDENCE = 0.5   # Default: 0.5, Range: 0.0-1.0
```

### Changing Cooldown Time

In `gesture_control_standalone.py`:

```python
GESTURE_COOLDOWN = 1.0  # Change this (in seconds)
```

---

## Troubleshooting

### Camera Issues

#### ‚ùå "Error: Could not open camera!"

**Solutions:**
1. **Check if camera is in use:**
   - Close Zoom, Skype, Teams, FaceTime, etc.
   - Check Activity Monitor (Mac) / Task Manager (Windows) for camera-using apps

2. **Try different camera index:**
   
   Edit `config.py`:
   ```python
   CAMERA_INDEX = 1  # Try 0, 1, 2, etc.
   ```

3. **Check camera permissions (macOS):**
   - System Preferences ‚Üí Security & Privacy ‚Üí Camera
   - Ensure Terminal has camera access

4. **Test camera independently:**
   ```bash
   # macOS: Open Photo Booth
   open -a "Photo Booth"
   
   # Or use this Python test:
   python -c "import cv2; cv2.VideoCapture(0).read()"
   ```

---

### Gesture Detection Issues

#### ‚ùå Gesture not detected / always shows "UNKNOWN"

**Solutions:**

1. **Improve lighting:**
   - Face a window or light source
   - Avoid backlighting (light behind you)
   - Use desk lamp if needed

2. **Hand position:**
   - Keep hand centered in frame
   - Distance: ~30-60cm from camera
   - Show full hand (all fingers visible)

3. **Gesture clarity:**
   - Extend fingers fully for OPEN_PALM
   - Close fist tightly for CLOSED_FIST
   - Isolate index finger for POINTING

4. **Lower confidence threshold:**
   
   Edit `config.py`:
   ```python
   MIN_DETECTION_CONFIDENCE = 0.3  # Lower from 0.5
   ```

---

### Action/Control Issues

#### ‚ùå Gesture detected but action doesn't trigger

**Solutions:**

1. **Check cooldown:**
   - Wait 1 second between gestures
   - Reduce cooldown in code if needed

2. **Platform-specific keys:**
   
   **Windows users**: Modify key names in `gesture_control_standalone.py`:
   ```python
   # macOS uses different key names than Windows
   # If 'volumeup' doesn't work, try:
   pyautogui.press('volumeup')  # or
   pyautogui.hotkey('ctrl', 'up')  # Custom shortcut
   ```

3. **Test PyAutoGUI manually:**
   ```python
   python -c "import pyautogui; pyautogui.press('volumeup')"
   ```

4. **Check system preferences:**
   - Ensure media keys are not disabled
   - Check if F-keys require Fn press (macOS)

#### ‚ùå "Gesture detected but repeated too many times"

**Solution:**

Increase cooldown in `gesture_control_standalone.py`:
```python
GESTURE_COOLDOWN = 2.0  # Increase from 1.0
```

---

### Performance Issues

#### ‚ùå Low FPS / Laggy camera feed

**Solutions:**

1. **Close other applications:**
   - Free up CPU/RAM
   - Close browser tabs, heavy apps

2. **Reduce camera resolution:**
   
   Edit `gesture_control_standalone.py`, add after `cap = cv2.VideoCapture(CAMERA_INDEX)`:
   ```python
   cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
   cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
   ```

3. **Disable pose tracking (hand-only mode):**
   
   Comment out pose drawing in `main()`:
   ```python
   # mp_drawing.draw_landmarks(
   #     image,
   #     results.pose_landmarks,
   #     mp_holistic.POSE_CONNECTIONS,
   #     landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())
   ```

4. **Check system resources:**
   - MediaPipe is GPU-accelerated when available
   - Ensure graphics drivers are updated

---

### Installation Issues

#### ‚ùå "ModuleNotFoundError: No module named 'mediapipe'"

**Solution:**

```bash
# Make sure conda environment is activated
conda activate lauzhack

# Reinstall dependencies
pip install -r requirements.txt

# If still failing, install manually:
pip install mediapipe opencv-python pyautogui
```

#### ‚ùå "Command 'conda' not found"

**Solution:**

Install Conda:
- **Miniconda** (lightweight): https://docs.conda.io/en/latest/miniconda.html
- **Anaconda** (full distribution): https://www.anaconda.com/

Or use Python virtual environment instead:
```bash
python -m venv venv
source venv/bin/activate  # Mac/Linux
# venv\Scripts\activate   # Windows

pip install -r requirements.txt
python gesture_control_standalone.py
```

---

## FAQ

### Q: How do I run this on startup?

**macOS:**
1. Create a shell script `start_gesture_control.sh`:
   ```bash
   #!/bin/bash
   cd /Users/matte/MDS/Personal/lauzhack/GestureControlPlugin/gesture_engine
   source ~/anaconda3/bin/activate lauzhack
   python gesture_control_standalone.py
   ```

2. Make executable: `chmod +x start_gesture_control.sh`

3. Add to Login Items:
   - System Preferences ‚Üí Users & Groups ‚Üí Login Items
   - Add the script

**Windows:**
- Create a batch file and add to Startup folder
- Or use Task Scheduler

---

### Q: Can I run this without the camera window?

Yes! Modify `gesture_control_standalone.py`:

```python
# Comment out the cv2.imshow line:
# cv2.imshow(WINDOW_NAME, image)

# And remove the window wait:
if cv2.waitKey(5) & 0xFF == 27:
    break
```

Note: You won't have visual feedback, but gestures will still work.

---

### Q: Does this work with multiple hands?

Currently, it prioritizes the right hand, then falls back to left hand. To support both hands simultaneously, you'd need to modify the gesture detection logic.

---

### Q: How accurate is the gesture detection?

With good lighting and clear gestures, accuracy is **90-95%**. MediaPipe is a production-grade model used in many commercial applications.

---

### Q: Can I use this for gaming?

Yes! Map gestures to game controls:

```python
elif gesture == "OPEN_PALM":
    pyautogui.press('w')  # Move forward
elif gesture == "CLOSED_FIST":
    pyautogui.press('space')  # Jump
```

Note: Some anti-cheat systems may flag automated inputs.

---

### Q: What about privacy/security?

- **All processing is local** - no data sent to servers
- Camera feed never leaves your computer
- MediaPipe model runs entirely offline
- You can disconnect internet and it still works

---

### Q: How do I add gestures for specific apps?

Use window focus detection:

```python
import pygetwindow as gw

def handle_gesture_action(gesture):
    # Get active window
    try:
        active_window = gw.getActiveWindow()
        if active_window and "Spotify" in active_window.title:
            # Spotify-specific controls
            if gesture == "OPEN_PALM":
                pyautogui.press('playpause')
        elif active_window and "Chrome" in active_window.title:
            # Chrome-specific controls
            if gesture == "OPEN_PALM":
                pyautogui.hotkey('command', 't')  # New tab
    except:
        pass  # Fallback to default actions
```

---

## Project Structure

```
lauzhack/
‚îú‚îÄ‚îÄ GestureControlPlugin/
‚îÇ   ‚îú‚îÄ‚îÄ gesture_engine/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                          # Original WebSocket version
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gesture_control_standalone.py    # ‚≠ê Standalone version (use this!)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                        # Configuration settings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îÇ   ‚îî‚îÄ‚îÄ src/                                 # C# plugin (not needed for standalone)
‚îú‚îÄ‚îÄ docs/                                    # Detailed documentation
‚îú‚îÄ‚îÄ GUIDE.md                                 # This file
‚îî‚îÄ‚îÄ README.md                                # Project overview
```

---

## Next Steps

1. ‚úÖ **Test the system** with the three default gestures
2. üé® **Customize actions** to match your workflow
3. ‚ûï **Add new gestures** for more controls
4. üöÄ **Set up autostart** for convenience
5. üìù **Share feedback** and improvements!

---

## Getting Help

If you encounter issues not covered in this guide:

1. **Check the logs**: Terminal output shows detailed error messages
2. **Review documentation**: See `docs/` folder for technical details
3. **Test components individually**:
   - Camera: `python -c "import cv2; cv2.VideoCapture(0)"`
   - MediaPipe: `python -c "import mediapipe; print('OK')"`
   - PyAutoGUI: `python -c "import pyautogui; pyautogui.press('volumeup')"`

---

**üéâ Enjoy your gesture-controlled system!**

Made with ‚ù§Ô∏è by the Lauzhack Team
